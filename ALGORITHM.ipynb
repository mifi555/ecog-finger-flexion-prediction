{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCz15UtNjyeS"
      },
      "source": [
        "# TEAM: BUDGET NEURALINK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns9L1QVFevPN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from scipy.stats import pearsonr\n",
        "from scipy import signal\n",
        "from scipy.io import loadmat\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import entropy\n",
        "from scipy.signal import welch\n",
        "from scipy.integrate import simps\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "from scipy.io import savemat\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfdzdXvHf-_S"
      },
      "source": [
        "# Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msJFHdN-f-hb"
      },
      "outputs": [],
      "source": [
        "raw_training_data = loadmat('raw_training_data.mat')\n",
        "testing_data = loadmat('leaderboard_data.mat')\n",
        "testing_data = testing_data['leaderboard_ecog']\n",
        "train_dg = raw_training_data['train_dg']\n",
        "train_ecog = raw_training_data['train_ecog']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-8noMD8gNTB"
      },
      "outputs": [],
      "source": [
        "def filter_data(raw_eeg, lowcut=1, highcut=150, filter_order=5, fs=1000):\n",
        "    \"\"\"\n",
        "    Write a filter function to clean underlying data.\n",
        "    Filter type and parameters are up to you. Points will be awarded for reasonable filter type, parameters and application.\n",
        "    Please note there are many acceptable answers, but make sure you aren't throwing out crucial data or adversly\n",
        "    distorting the underlying data!\n",
        "\n",
        "    Input:\n",
        "      raw_eeg (samples x channels): the raw signal\n",
        "      fs: the sampling rate (1000 for this dataset)\n",
        "    Output:\n",
        "      clean_data (samples x channels): the filtered signal\n",
        "    \"\"\"\n",
        "\n",
        "    # Butter function to create a bandpass filter\n",
        "    b, a = signal.butter(filter_order, [lowcut, highcut], btype='band', fs=fs)\n",
        "\n",
        "    # Filtfilt to apply the filter to the data: avoids phase distortion\n",
        "    clean_data = signal.filtfilt(b, a, raw_eeg, axis=0)\n",
        "\n",
        "    return clean_data\n",
        "\n",
        "def calculate_number_of_windows(num_samples, fs, window_length_ms, window_overlap_ms):\n",
        "    # Convert milliseconds to seconds\n",
        "    window_length = window_length_ms / 1000.0\n",
        "    window_overlap = window_overlap_ms / 1000.0\n",
        "\n",
        "    # Calculate the step size in seconds\n",
        "    step_size = window_length - window_overlap\n",
        "\n",
        "    # Calculate the total duration of the data in seconds\n",
        "    total_duration = num_samples / fs\n",
        "\n",
        "    # Calculate the number of windows\n",
        "    M = 1 + np.ceil((total_duration - window_length) / step_size)\n",
        "\n",
        "    return M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibOyuU7NgZEQ"
      },
      "outputs": [],
      "source": [
        "#line length\n",
        "line_length = lambda x : np.sum(np.absolute(np.ediff1d(x)))\n",
        "\n",
        "#area\n",
        "area = lambda x : np.sum(np.abs(x))\n",
        "\n",
        "#energy\n",
        "energy = lambda x : np.sum(np.square(x))\n",
        "\n",
        "#dero crossings\n",
        "zero_crossings = lambda x: np.sum((x[:-1] - x.mean() > 0) & (x[1:] - x.mean() < 0)) + np.sum((x[:-1] - x.mean() < 0) & (x[1:] - x.mean() > 0))\n",
        "\n",
        "#mean voltage\n",
        "def mean_voltage(channel):\n",
        "    return np.mean(channel)\n",
        "\n",
        "#signal entropy\n",
        "def signal_entropy(channel):\n",
        "    \"\"\" Calculate the entropy of the signal \"\"\"\n",
        "    hist, bin_edges = np.histogram(channel, bins=64, density=True)\n",
        "    return entropy(hist)\n",
        "\n",
        "#hjorth activity\n",
        "def hjorth_activity(channel):\n",
        "    \"\"\" Calculate Hjorth Activity \"\"\"\n",
        "    return np.var(channel)\n",
        "\n",
        "#hjorth mobility\n",
        "def hjorth_mobility(channel):\n",
        "    \"\"\" Calculate Hjorth Mobility \"\"\"\n",
        "    return np.sqrt(np.var(np.diff(channel)) / np.var(channel))\n",
        "\n",
        "#hjorth complexity\n",
        "def hjorth_complexity(channel):\n",
        "    \"\"\" Calculate Hjorth Complexity \"\"\"\n",
        "    return hjorth_mobility(np.diff(channel)) / hjorth_mobility(channel)\n",
        "\n",
        "def spectral_power(channel, fs, band):\n",
        "    \"\"\"\n",
        "    Calculate the spectral power within a specific frequency band using Welch's method.\n",
        "\n",
        "    Args:\n",
        "        channel (array): EEG signal.\n",
        "        fs (int): Sampling frequency.\n",
        "        band (tuple): Frequency band (low_freq, high_freq).\n",
        "\n",
        "    Returns:\n",
        "        float: Spectral power in the given frequency band.\n",
        "    \"\"\"\n",
        "    f, Pxx = welch(channel, fs=fs, nperseg=len(channel))\n",
        "    band_mask = (f >= band[0]) & (f <= band[1])\n",
        "    return np.sum(Pxx[band_mask])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7xapnnZgbuF"
      },
      "outputs": [],
      "source": [
        "def get_features(filtered_window, fs=1000):\n",
        "    \"\"\"\n",
        "      Write a function that calculates features for a given filtered window.\n",
        "      Feel free to use features you have seen before in this class, features that\n",
        "      have been used in the literature, or design your own!\n",
        "\n",
        "      Input:\n",
        "        filtered_window (window_samples x channels): the window of the filtered ecog signal\n",
        "        fs: sampling rate\n",
        "      Output:\n",
        "        features (channels x num_features): the features calculated on each channel for the window\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    bands = [(8, 12), (18, 24), (75, 115), (125, 159), (160, 175)]\n",
        "\n",
        "    for channel in filtered_window.T:\n",
        "      channel_features = [\n",
        "          line_length(channel),\n",
        "          area(channel),\n",
        "          energy(channel),\n",
        "          zero_crossings(channel),\n",
        "          mean_voltage(channel),\n",
        "          signal_entropy(channel),\n",
        "          hjorth_activity(channel),\n",
        "          hjorth_mobility(channel),\n",
        "          hjorth_complexity(channel)\n",
        "      ]\n",
        "\n",
        "      spectral_features = [spectral_power(channel, fs, band) for band in bands]\n",
        "      channel_features.extend(spectral_features)\n",
        "\n",
        "      features.append(channel_features)\n",
        "\n",
        "    return np.array(features)\n",
        "\n",
        "def get_windowed_feats(raw_ecog, fs, window_length, window_overlap):\n",
        "    \"\"\"\n",
        "      Write a function which processes data through the steps of filtering and\n",
        "      feature calculation and returns features. Points will be awarded for completing\n",
        "      each step appropriately (note that if one of the functions you call within this script\n",
        "      returns a bad output, you won't be double penalized). Note that you will need\n",
        "      to run the filter_data and get_features functions within this function.\n",
        "\n",
        "      Inputs:\n",
        "        raw_eeg (samples x channels): the raw signal\n",
        "        fs: the sampling rate (1000 for this dataset)\n",
        "        window_length: the window's length\n",
        "        window_overlap: the window's overlap\n",
        "      Output:\n",
        "        all_feats (num_windows x (channels x features)): the features for each channel for each time window\n",
        "          note that this is a 2D array.\n",
        "    \"\"\"\n",
        "    filtered_ecog_data = filter_data(raw_ecog)\n",
        "\n",
        "    #number of samples in each window\n",
        "    window_length_samples = int(window_length * fs)\n",
        "\n",
        "    #number of samples that should overlap\n",
        "    window_overlap_samples = int(window_overlap * fs)\n",
        "\n",
        "    #step size:  number of samples by which we move the window forward to create the next window.\n",
        "    step_size = window_length_samples - window_overlap_samples\n",
        "\n",
        "    total_samples = raw_ecog.shape[0]\n",
        "    #feature windows\n",
        "\n",
        "    all_features = []\n",
        "\n",
        "    for start in range(0, total_samples - window_length_samples + 1, step_size):\n",
        "      window_start = filtered_ecog_data[start : (start + window_length_samples), :]\n",
        "\n",
        "      #features for each channel within a window, which results in a two-dimensional array (or matrix)\n",
        "      #each row corresponds to a different channel and each column corresponds to a different feature extracted from that channel\n",
        "      window_features = get_features(window_start)\n",
        "\n",
        "      all_features.append(window_features)\n",
        "\n",
        "    return all_features\n",
        "\n",
        "def create_R_matrix(features, N_wind):\n",
        "    #pad the beginning of the features matrix with copies of the first N_wind-1 rows\n",
        "    padding = features[:N_wind-1]\n",
        "    padded_features = np.vstack((padding, features))\n",
        "\n",
        "    #initialize an empty list to hold the rows of the R matrix\n",
        "    R_list = []\n",
        "\n",
        "    #iterate over the padded features matrix to construct the R matrix rows\n",
        "    for i in range(N_wind-1, len(padded_features)):\n",
        "        #extract the current window and N_wind-1 preceding windows\n",
        "        window_features = padded_features[i-N_wind+1:i+1].flatten()\n",
        "        #prepend a 1 to the feature vector to account for the bias term\n",
        "        window_features_with_bias = np.insert(window_features, 0, 1)\n",
        "        #append to the R_list\n",
        "        R_list.append(window_features_with_bias)\n",
        "\n",
        "    #convert the R_list to a numpy array to form the R matrix\n",
        "    R = np.array(R_list)\n",
        "\n",
        "    return R\n",
        "\n",
        "def downsample_data(data, num_windows):\n",
        "    #number of finger features\n",
        "    num_features = data.shape[1]\n",
        "    #store downsampled data\n",
        "    downsampled_data = np.zeros((num_windows, num_features))\n",
        "\n",
        "    #iterate through each feature\n",
        "    for i in range(num_features):\n",
        "        #data for each finger\n",
        "        feature_data = data[:, i]\n",
        "\n",
        "        #create a new time axis for interpolation, matching the target number of windows\n",
        "        new_time_axis = np.linspace(0, len(feature_data) - 1, num=num_windows)\n",
        "\n",
        "        downsampled_data[:, i] = np.interp(new_time_axis, np.arange(len(feature_data)), feature_data)\n",
        "\n",
        "    return downsampled_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2qfZat9g2s3",
        "outputId": "78a9be5d-e9a8-4b79-87d9-cedf879a5b5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "Train done\n",
            "Test done\n",
            "---\n",
            "1\n",
            "Train done\n",
            "Test done\n",
            "---\n",
            "2\n",
            "Train done\n",
            "Test done\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# setting up data\n",
        "N_wind = 10\n",
        "fs = 1000\n",
        "window_length = 0.1\n",
        "window_overlap = 0.05\n",
        "test_sz = 0.2\n",
        "\n",
        "ecog_train1, ecog_test1, glove_train1, glove_test1 = train_test_split(train_ecog[0][0], train_dg[0][0], test_size=test_sz, shuffle=False)\n",
        "ecog_train2, ecog_test2, glove_train2, glove_test2 = train_test_split(train_ecog[1][0], train_dg[1][0], test_size=test_sz, shuffle=False)\n",
        "ecog_train3, ecog_test3, glove_train3, glove_test3 = train_test_split(train_ecog[2][0], train_dg[2][0], test_size=test_sz, shuffle=False)\n",
        "\n",
        "ecog_trains = [ecog_train1, ecog_train2, ecog_train3]\n",
        "ecog_tests = [ecog_test1, ecog_test2, ecog_test3]\n",
        "\n",
        "subject_train_feats = []\n",
        "subject_test_feats = []\n",
        "for subject in range(3):\n",
        "    print(subject)\n",
        "    subject_train_feats.append(get_windowed_feats(ecog_trains[subject], fs, window_length, window_overlap))\n",
        "    print(\"Train done\")\n",
        "    subject_test_feats.append(get_windowed_feats(ecog_tests[subject], fs, window_length, window_overlap))\n",
        "    print(\"Test done\")\n",
        "    print(\"---\")\n",
        "\n",
        "subjects_data = [\n",
        "    (subject_train_feats[0], glove_train1, subject_test_feats[0], glove_test1),\n",
        "    (subject_train_feats[1], glove_train2, subject_test_feats[1], glove_test2),\n",
        "    (subject_train_feats[2], glove_train3, subject_test_feats[2], glove_test3)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1x3y9TerXjRQ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def save_data(data, filename):\n",
        "    \"\"\"Save data to a file using pickle.\"\"\"\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "# Save the subjects_data to a file\n",
        "save_data(subjects_data, 'subjects_data.pkl')\n",
        "\n",
        "def load_data(filename):\n",
        "    \"\"\"Load data from a file using pickle.\"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Load the subjects_data from a file\n",
        "subjects_data = load_data('subjects_data.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2ACV4u3if4y"
      },
      "source": [
        "# ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RZnp-r9ai6ug"
      },
      "outputs": [],
      "source": [
        "def mean_squared_error(actual, predicted):\n",
        "    \"\"\"\n",
        "    Calculate the mean squared error between actual and predicted values\n",
        "\n",
        "    Parameters:\n",
        "    actual (numpy array): The actual values\n",
        "    predicted (numpy array): The predicted values by the model\n",
        "\n",
        "    Returns:\n",
        "    float: The mean squared error\n",
        "    \"\"\"\n",
        "    #calculate the difference between actual and predicted values\n",
        "    difference = actual - predicted\n",
        "\n",
        "    #square the differences\n",
        "    squared_difference = difference ** 2\n",
        "\n",
        "    #calculate the mean/average of the squared differences\n",
        "    mse = squared_difference.mean()\n",
        "\n",
        "    return mse\n",
        "\n",
        "def calculate_correlations(Y_test, Y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the Pearson correlation coefficient between actual and predicted\n",
        "    finger angles for each finger.\n",
        "\n",
        "    Parameters:\n",
        "    Y_test (numpy array): Actual test finger angles, shape (num_samples, num_fingers)\n",
        "    Y_pred (numpy array): Predicted finger angles, shape (num_samples, num_fingers)\n",
        "\n",
        "    Returns:\n",
        "    List of Pearson correlation coefficients for each finger.\n",
        "    \"\"\"\n",
        "    correlations = []\n",
        "    #iterate through each finger\n",
        "    for i in range(Y_test.shape[1]):\n",
        "        corr, _ = pearsonr(Y_test[:, i], Y_pred[:, i])\n",
        "        correlations.append(corr)\n",
        "    return correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNqiHv4LjIXW"
      },
      "source": [
        "## Model: LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DjqnQA36jKt0"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_lgbm(features_train, targets_train, features_test):\n",
        "    # Initialize a list to hold models for each dimension\n",
        "    models = []\n",
        "    # Initialize an array to hold predictions, corrected to match the test features shape\n",
        "    predictions = np.zeros((features_test.shape[0], targets_train.shape[1]))\n",
        "\n",
        "    # Train a separate model for each dimension of the target\n",
        "    for dim in range(targets_train.shape[1]):\n",
        "        model = lgb.LGBMRegressor(objective='regression', n_estimators=80, learning_rate=0.05, num_leaves=10, random_state=42)\n",
        "        model.fit(features_train, targets_train[:, dim])\n",
        "        predictions[:, dim] = model.predict(features_test)\n",
        "        models.append(model)\n",
        "\n",
        "    return models, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xXutIIqkJYF"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This code was used for hyperparam tuning of the number of leaves\n",
        "\n",
        "num_leaves_values = [10, 20, 30, 40, 50]\n",
        "average_correlations = []\n",
        "\n",
        "for num_leaves in num_leaves_values:\n",
        "    correlations_across_subjects = []\n",
        "    for i, (windowed_feats, glove_train, windowed_feats_test, glove_test) in enumerate(full_subjects_data, start=1):\n",
        "        R_train = create_R_matrix(windowed_feats, N_wind)\n",
        "        downsampled_finger_flexion = downsample_data(glove_train, R_train.shape[0])\n",
        "        R_test = create_R_matrix(windowed_feats_test, N_wind)\n",
        "\n",
        "        lgbm_model, Y_pred = train_and_evaluate_lgbm(R_train, downsampled_finger_flexion, R_test, num_leaves)\n",
        "        Y_test = downsample_data(glove_test, len(windowed_feats_test))\n",
        "\n",
        "        # Evaluation\n",
        "        correlations = calculate_correlations(Y_test, Y_pred)\n",
        "        print(\"avg corr for sub {i} for this val of num leveas = \", np.mean(correlations))\n",
        "        correlations_across_subjects.append(np.mean(correlations))\n",
        "\n",
        "    # Calculate the average correlation for this num_leaves setting across all subjects\n",
        "    avg_correlation = np.mean(correlations_across_subjects)\n",
        "    average_correlations.append(avg_correlation)\n",
        "    print(f\"Average Correlation for num_leaves={num_leaves}: {avg_correlation}\")\n",
        "\n",
        "# Plotting the number of leaves against the average correlation\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(num_leaves_values, average_correlations, marker='o')\n",
        "plt.title('Number of Leaves vs. Average Correlation')\n",
        "plt.xlabel('Number of Leaves')\n",
        "plt.ylabel('Average Correlation')\n",
        "plt.grid(True)\n",
        "plt.show()```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gDbYxr1zjPvY",
        "outputId": "74f63948-7f99-4aeb-f3d7-94b87d3ddda8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.925186 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2064250\n",
            "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 8680\n",
            "[LightGBM] [Info] Start training from score -0.091412\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.310442 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2064250\n",
            "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 8680\n",
            "[LightGBM] [Info] Start training from score -0.026955\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.231193 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2064250\n",
            "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 8680\n",
            "[LightGBM] [Info] Start training from score -0.043684\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.854628 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2064250\n",
            "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 8680\n",
            "[LightGBM] [Info] Start training from score -0.159422\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.437831 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2064250\n",
            "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 8680\n",
            "[LightGBM] [Info] Start training from score 0.037508\n",
            "Mean Squared Error (LightGBM) for Subject 1: 0.7945517185708829\n",
            "Correlations for Subject 1: [0.5756733398463739, 0.6460747552467837, -0.01450111208738237, 0.6684531635942598, -0.2374637081061217]\n",
            "Average Correlation for Subject 1: 0.32764728769878265\n",
            "\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.667548 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1598430\n",
            "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 6720\n",
            "[LightGBM] [Info] Start training from score -0.182853\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.050441 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1598430\n",
            "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 6720\n",
            "[LightGBM] [Info] Start training from score -0.187921\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.520623 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1598430\n",
            "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 6720\n",
            "[LightGBM] [Info] Start training from score 0.154790\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.733266 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1598430\n",
            "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 6720\n",
            "[LightGBM] [Info] Start training from score -0.035617\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.680920 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1598430\n",
            "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 6720\n",
            "[LightGBM] [Info] Start training from score -0.064930\n",
            "Mean Squared Error (LightGBM) for Subject 2: 0.7480766488256531\n",
            "Correlations for Subject 2: [0.5912702329821206, 0.38508481287379864, 0.32117724584180624, 0.5460165909036535, 0.4287482530783145]\n",
            "Average Correlation for Subject 2: 0.4544594271359387\n",
            "\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.293220 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2132940\n",
            "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 8960\n",
            "[LightGBM] [Info] Start training from score 0.078358\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.140871 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2132940\n",
            "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 8960\n",
            "[LightGBM] [Info] Start training from score 0.105185\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.135613 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2132940\n",
            "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 8960\n",
            "[LightGBM] [Info] Start training from score 0.016826\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.318456 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2132940\n",
            "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 8960\n",
            "[LightGBM] [Info] Start training from score 0.208178\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.056826 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2132940\n",
            "[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 8960\n",
            "[LightGBM] [Info] Start training from score -0.095914\n",
            "Mean Squared Error (LightGBM) for Subject 3: 0.49877608256929923\n",
            "Correlations for Subject 3: [0.7530831722117936, 0.3946403140411183, 0.5625704985268651, 0.6280171328182431, 0.6076304470391045]\n",
            "Average Correlation for Subject 3: 0.589188312927425\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "models = []\n",
        "\n",
        "for i, (windowed_feats, glove_train, windowed_feats_test, glove_test) in enumerate(subjects_data, start=1):\n",
        "\n",
        "    R_train = create_R_matrix(windowed_feats, N_wind)\n",
        "    downsampled_finger_flexion = downsample_data(glove_train, R_train.shape[0])\n",
        "\n",
        "    R_test = create_R_matrix(windowed_feats_test, N_wind)\n",
        "\n",
        "    # Train using LightGBM\n",
        "    lgbm_model, Y_pred = train_and_evaluate_lgbm(R_train, downsampled_finger_flexion, R_test)\n",
        "    Y_test = downsample_data(glove_test, len(windowed_feats_test))\n",
        "\n",
        "    # Evaluation\n",
        "    mse = mean_squared_error(Y_test, Y_pred)\n",
        "    correlations = calculate_correlations(Y_test, Y_pred)\n",
        "    results.append((mse, correlations))\n",
        "\n",
        "    # Save the trained model\n",
        "    joblib.dump(lgbm_model, f'lgbm_model_subject_{i}.joblib')\n",
        "    models.append(lgbm_model)\n",
        "\n",
        "    print(f\"Mean Squared Error (LightGBM) for Subject {i}: {mse}\")\n",
        "    print(f\"Correlations for Subject {i}: {correlations}\")\n",
        "    print(f\"Average Correlation for Subject {i}: {np.mean(correlations)}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD4iRtk8jUJJ"
      },
      "source": [
        "# Submit Leaderboard.mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FvXnsCmmjY3L"
      },
      "outputs": [],
      "source": [
        "# this method works well with lgbm\n",
        "def predict_with_pretrained_models(models, features_test):\n",
        "    # Assuming models is a list of pre-trained LightGBM models, one for each target dimension\n",
        "    num_dimensions = len(models)  # Number of target dimensions, e.g., number of fingers\n",
        "    num_samples = features_test.shape[0]\n",
        "\n",
        "    # Initialize an array to hold predictions for each dimension\n",
        "    predictions = np.zeros((num_samples, num_dimensions))\n",
        "\n",
        "    # Generate predictions for each dimension using the corresponding pre-trained model\n",
        "    for dim, model in enumerate(models):\n",
        "        predictions[:, dim] = model.predict(features_test)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def interpolate_predictions(predictions, target_length):\n",
        "    # Current length of the predictions\n",
        "    current_length = predictions.shape[0]\n",
        "    # Target length - 147,500\n",
        "    # Create an array representing the indices of the current predictions\n",
        "    current_indices = np.linspace(0, current_length - 1, num=current_length)\n",
        "    # Create an array representing the indices for the interpolated output\n",
        "    target_indices = np.linspace(0, current_length - 1, num=target_length)\n",
        "    # Interpolate predictions to target length for each dimension\n",
        "    interpolated_predictions = np.zeros((target_length, predictions.shape[1]))\n",
        "    for i in range(predictions.shape[1]):  # Interpolate for each feature/dimension\n",
        "        interpolated_predictions[:, i] = np.interp(target_indices, current_indices, predictions[:, i])\n",
        "    return interpolated_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28x7xsJMjiWD"
      },
      "outputs": [],
      "source": [
        "# currently this submits our lgbm model\n",
        "predicted_dg = np.empty((3, 1), dtype=object)\n",
        "\n",
        "for i in range(3):\n",
        "    print(i)\n",
        "    models = joblib.load(f'lgbm_model_subject_{i+1}.joblib')\n",
        "    ecog_test = testing_data[i][0]\n",
        "    windowed_feats_test = get_windowed_feats(ecog_test, fs, window_length, window_overlap)\n",
        "    R_test = create_R_matrix(windowed_feats_test, N_wind)\n",
        "\n",
        "    # Fetch predictions\n",
        "    predictions = np.zeros((R_test.shape[0], 5))\n",
        "    for dim, single_model in enumerate(models):\n",
        "        predictions[:, dim] = single_model.predict(R_test)\n",
        "\n",
        "    # Interpolate predictions to match the required 147,500 length\n",
        "    interpolated_predictions = interpolate_predictions(predictions, 147500)\n",
        "    predicted_dg[i, 0] = interpolated_predictions\n",
        "\n",
        "savemat('submission.mat', {'predicted_dg': predicted_dg})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}